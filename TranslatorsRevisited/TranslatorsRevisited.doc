/* -*- mode: html -*- */
/*! \mainpage Translators Revisited


Hi Dimitri,

I tried to remove Config_getBool() from the translator methods, and
I was successful to do it in the way that will be painless for both
you and for the language maintainers now.  The following sources
were modified: classdef.cpp, classlist.cpp, index.cpp, optimc.cpp,
packagedef.cpp, rtfgen.cpp.  The optimc.h and optimc.cpp were added.
The modification is very simple and errorproof.  
See \ref solution_optimc "details of the solution".

The translator.h, translator_en.h, and translator_adapter.h were
updated.  The other translator_xx.h were changed only to derive from
the TranslatorAdapterCVS instead of the Translator.

This is another step towards new implementation of translators.
If you accept this, the language maintainers should be sent some
explanation. ("Damn, why it was changed again?" ;-)  Possibly, the
time has come to send this document to doxygen-develop.

See you,
Petr
   
<hr>
This document suggests new implementation or replacement of
Translator based classes for producing language-specific strings,
sentences, and information. It is a subject of the discussion.  Some
implementation steps were already done.

\author Petr Prikryl
\author Dimitri van Heesch

\section Changes Changes

(Last changes first.)

\par 2001/09/23
 - \ref PreparingDoxygen updated.  The Config_getBool() removed 
   from the translator methods -- documented 
   \ref solution_optimc "here".  The following core sources were
   changed: classdef.cpp, classlist.cpp, index.cpp, optimc.cpp,
   packagedef.cpp, rtfgen.cpp.
 - \ref motivation updated.
 - \ref questions updated.
 - \ref EntityApproach slightly updated.
 - Minor updates elsewhere.
 
\par 2001/09/17
 - More details related to \ref IApplicTemplate.  Processing
   instructions introduced.
 - New subsection: \ref PreparingDoxygen 
 - \ref DefEntityFileSyntax updated.

\par 20001/09/16
 - Title changed
 - First step towards XML-like entity definitions.  The syntax of
   entity definition can be taken from XML without changes.
   Examples in this document updated.
 - Conversion program updated; C variants of texts added.
 - \ref DefEntityFileSyntax updated.
 - More details added to \ref SupportScript.


\section TOC Table of contents
   
 - \ref motivation
 - \ref questions
 - \ref CurrentMechanismAnalysis
 - \ref EntityApproach
 - \ref LanguageRelated
 - \ref LanguageIndependent
 - \ref GenerDoxEntities
 - \ref DevelopmentStages
     - \ref StudyXML
     - \ref FinishingThis
     - \ref PreparingDoxygen 
     - \ref FindDevelopers
     - \ref DefEntityFileSyntax
     - \ref IDoxEntGenerator
     - \ref TranslatorExp
     - \ref ISimpleEnt
     - \ref ISimpleEntReplacement
     - \ref ITemplateEnt
     - \ref IApplicTemplate
     - \ref NewTranslator
     - \ref SupportScript     
     - \ref RemovingTranslator

\section motivation Motivation

Language support is now implemented via classes based on the
abstract class \c Translator.  The main reason is that, generally,
it is difficult to produce nicely readable sentences from \e
fragments of the chosen language.  

It is impossible to generalize the process of gluing the fragments
together independently on the language. It's because the language
rules cannot be generally algoritmized.  Often, the whole sentences
must be generated.  It was found easier to do it programmatically,
because the language maintainer can modify the sentence according to
the passed arguments. 

Basically, the generated sequences may contain document-related
generated parts, e.g. single identifiers or lists of identifiers.
The generated sentences and fragments may be modified via argument
values like \c first_capital, \c plural, etc.

On the other hand, it would be much easier for the language
maintainers or even for end users to maintain the language via
editing text files, without need to recompile whole doxygen.

The idea is based on implementing a mechanism that will use 
<em>templates for whole sentences</em>.  The fundamental question is
<b>How?</b>


\section questions The questions

Here are the more detailed questions.  The questions were already
partly answered, and some implementation work just begun to proof
the design.

<dl>
<dt>\anchor Q1 <b>Q1: Can the language information be moved from
  code to files?</b>
<dd>Can any new language be introduced without need for compilation of
  doxygen?  This question will be answered \e yes or \e no,
  depending on the answer to \ref Q2 "the second question".

<dt>\anchor Q2 <b>Q2: How the mechanism should look like?</b>
<dd>Some work to answer that was already done. See
  the \ref CurrentMechanismAnalysis and \ref EntityApproach.

<dt>\anchor Q3 <b>Q3: Is it possible to implement the mechanism 
  with reasonable effort in reasonable time?</b>
<dd>In other words, we should be rather conservative (feet on the
  ground).  If the mechanism will not be clearly implementable, it
  should not be implemented at all. 
  
  The \ref EntityApproach "Entity based approach" seems to be
  acceptable and implementable. One of the main purposes of this
  document is to discuss the details or to find the problems.
  Part of the answer is based on \ref GenerDoxEntities.
  
<dt>\anchor Q4 <b>Q4: Can the mechanism be introduced gradually 
  to doxygen?</b>
<dd>It definitely is important to be able to test the new
  implementation and possibly compare the results with those
  generated using the old approach.  It must also be possible to
  implement some laguages in the old way and some in the new way --
  at least during testing, until the results will be the same (or
  very acceptable).
  
  The answer is roughly sketched in the 
  \ref DevelopmentStages "description of the development stages".
  
<dt>\anchor Q5 <b>Q5: Can the language maintenance be supported 
  somehow?</b>
<dd>Since the introduction of the \c TranslatorAdapter classes, 
  the language maintaintenance and the core development could be \e
  decoupled.  Any language maintainer could not slow-down the core
  development by not implementing something which is required for
  compilation.  This worked also in the pre-adapter era.  But after
  introduction of the translator adapters, the translator methods 
  can change also the interface dramatically while still using the
  work done by the language maintainers for the older implementation
  of the language specific translations. 
  
  Moreover, the status of the language support can be discovered
  automatically. The \c translator.pl script is able to show the
  status of the language support for all languages.  It also
  produces \c translator_report.txt which lists what should be
  implemented (new things) or removed (obsolete things).  \ref
  SupportScript "Similar support" should be implemented also for the
  new mechanism.  
  
</dl>


\section CurrentMechanismAnalysis The analysis of the curent mechanism

The core of the solution will be based on the observation that
calling the \c trSimpleMethods() of the \c Translator based classes
can be seen as asking for the content of the references to the 
<em>language dependent</em> entities.  We can think about the code
inside such methods as about extremely simple algorithm that
replaces the entity reference (the call) to the simple text string.
The string is stored as a string literal near the code. The \c
trRelatedFunctionstrRelatedFunctions() can be the typical example.

Some of the \c trAlmostSimpleMethods() behave as if they contained
\e more entities.  If we consider the boolean like argument values
(for example) to be part of the name of the entity, then the method
contains that many entity definitions how many combinations of the
arguments are used.  Because of that, we can think about the code as
about some very specialized algorithm for selection of one entity
from the fixed set of entities.  The \c trFile() could be the
typical example, and the \c trFile(true,false), \c
trFile(false,true), and so on could be seen as special names of
internally selected entity definitions.  

The \c trMoreComplexMethods() generate a text string where certain
parts of the output were introduced through arguments as very
variable part of the content (like a line number or some
identifier). The variable content is always placed into well known
place inside the generated text.  The surrounding text is language
dependent.  If there is more than one variable argument, then also
the order of application of the arguments inside the generated text
may be language dependent.  So, we may think about the data content
of such a method as about some <em>template string with markers</em> for
placement the first, second, and-so-on arguments.  In the same time,
we may think about such a method as about a specialized algorithm
that gets the template and replaces the markers by variable
arguments.  The \c trCollaborationDiagram() could be the typical
example.  The \c trDefinedAtLineInSourceFile() could be another
example -- here the replacement of markers is done in the doxygen
generators (which is a bit closer to the final solution).  Both
examples could be converted to the other one.

As in the case of simple methods, we can think about a subset of the
complex methods as if they contained <em>a set of templates</em>.
Then, the alorithm implements also selection of the right template.

\anchor SummaryCurrentMechanism
To summarize the current mechanism, there are basically four types
of \c trMethods() that should be replaced by the new solution.
(There is one exception, the \c trWriteList(), but it can be moved
to the language independent part when we define three simple
language-dependent entities used as list-item separators.) In each
of the four cases, the \c trMethod() implements three basic things:

 -# <em>Data store</em> is implemented via string literals.
 -# <em>Selection mechanism</em> does access one entity or one entity
    template of some set.  In the simple case, the set contains only one
    element.  Then the selection mechanism is trivial.
 -# <em>Replacement mechanism</em> replaces special markers inside
    the template entity by a variable string argument.  In the case
    of simple entities, the mechanism is trivial (no operation).
    The result of replacement is then returned as the result of the
    method.

\section EntityApproach Entity based solution

The solution should transform the usage of \c trMethods() to
textual \c &trEntities; with definitions read from textual files.
With respect to the previous summary, the <em>data store</em>
will be changed from the string literals inside the code to the
external files.  This way the data store will be uncoupled from the
code.  As the files will probably be read only once, then the
complexity of the format is not a problem.  I feel that---also with
respect to growing importance of XML---the definition of entities
should be stored in XML oriented way.  Partly because the authors of
XML were thinking about many problems that we want to solve (and
they have already solved them), partly because growing comunity of
developers will understand what is going on, and partly because
there possibly are some libraries to cope with such files.

The <em>selection mechanism</em>, which is responsible for
retrieving the desired entity or the template entity will partly be
implemented via memory structures that will be filled with the
entity definitions read from the external data store (files).  In
general case, the memory structure should be map-like structure. The
entity (or template entity) definition will be accessed using the
key -- the entity name.  If there is some XML related library for
building and accessing entity definitions, then we should use it.

The <em>replacement mechanism</em> will implement searching for
template markers inside the content of a template entity and it will
replace it with the arguments.  The question still is, how the 
template arguments will be represented in the text together with the
template-entity reference (syntactically).  It seems that the 
<em>processing instructions</em> could be used in generated, 
language-independent XML intermediate document.  That could be
converted into the language dependent document of the chosen format 
using XML transformation tools.  But this is a long-term goal.
Until then, the translator class will be used to implement the part
that would be done by the interpretation of the processing
instructions.

Thinking about the entity approach, the replacement mechanism should
be also able to expand the entity references used inside another
entity definition.  Again, if there is some XML related library to
do such things, then we should use it.

To summarize the entity approach, the mechanism should generalize
what is already done by doxygen. and what is 
\ref SummaryCurrentMechanism "summarized" at the end of the
\ref CurrentMechanismAnalysis "analysis of the current mechanism".
So, it must implement the <em>data store</em>, the <em>selection
mechanism</em>, and the <em>replacement mechanism</em>.


\section LanguageRelated Language-related issues

The \ref EntityApproach "outline of the entity-based solution"
shows how the code of \c trMethods() could be separated from data
and how both the code and the data structure could be generalized.
However, it still does not say, how the language support should be
solved.

Apparently, all of the language-specific Translator-based classes
could be replaced using the mentioned mechanism.  Only the <em>data
store</em> part will be language dependent.  Instead of
instantiation of the chosen translator class, the files with entity
definitions for the chosen language will be loaded.  

The question is, whether it should happen similarly like in the
current version of doxygen, or whether we could find some advantage
related to the new approach.  Parts of the answer can be revealed
during next work on \ref DevelopmentStages.  Parts of the answer may
know you, because you know the internals of doxygen more than
anybody else. My guess is that we should postpone the
language-dependent processing of a document to as late time as
possible.  We should be oriented towards some internal, language
independent, and final output form independent intermediate format
that will be converted to language dependent form only by the
final-output generators.

Another important question is how to convert the work already done
by current language maintainers into definitions of the
language-dependent entities.  The problem was already stated 
in \ref Q3 "the third" and in \ref Q4 "the fourth" questions.
Some ideas and procedures are shown in \ref GenerDoxEntities
and \ref DevelopmentStages.


\section LanguageIndependent Language-independent issues

I guess that the generalized mechanism could be used also for
generators of the final output forms (HTML, RTF, LaTeX, etc.).

The more I think about it, the more it looks that many of the
problems that I am thinking about could be solved using pure XML as
the intermediate format and using XML transformation tools for
transformation of the intermediate form into the final output
formats.

On the other hand, I feel that we should do smaler steps to the
final solution than to remake everything into XML.  Surely, if
something is easy to implement in XML-like way, we should try to do
it this way.


\section GenerDoxEntities Automatic generation of doxygen entities

This section should give the answer to \ref Q3 "the third question".

It is possible to create a simple program that instantiates all
of the language translators.  Then it calls all the interesting
methods of the translator and writes the result into a textual file
like the following -- see below.  

If we can think about some argument of the \c trMethod() as about 
a hint for the <em>selection mechanism</em> 
(see \ref CurrentMechanismAnalysis "above"), then we could rewrite
the \c trMethod() into so many methods, how many entity definitions
could be recognized inside.  In other words, one \c trMethod() can
be converted to more than one entity definition.

If we can think about some argument as a variable argument, we can
use a marker when calling the \c trMethod() as the argument value.

I tried to create a very simplified program like that.  Look at the 
results below.  Pay attention to the roughly described categories
and notices, then go to \ref DoxEntSummary "summary".  Here is the
output generated using the English and the Dutch translators:

\verbinclude englishLang.ent

And now the same for Dutch:

\verbinclude dutchLang.ent


\subsection DoxEntSummary Summary of doxygen entity categories

The <em>simple entities</em> are also simple to implement.  If doxygen did
not use other kind of entities, then its implementation would
probably be entity oriented from the beginning (i.e. symbols defined
in textual files), wouldn't it be?

The key part are the <em>template entities</em>, like the 
\c &trCollaborationDiagram;. But see also that \c
&trDefinedAtLineInSourceFile; uses \c @0 and \c @1 markers. This is
exactly what I mean by the template entity.  So you definitely had
the same idea earlier ;-)

There is one stranger among the translator methods -- the
&trWriteList; which generates a list of separated markers.  Looking
more carefully, it is clear that there are only three types of
markers (English is typical):
  -# separator between the majority of a long list items (comma, space),
  -# separator between the last and previous items in a long list
     (comma, space, \c and, space),
  -# separator between items of a list with two elements (space, \c
     and, space -- no comma in English).
     
In Czech (and possibly in Dutch) the second and the third separators
are the same.  No problem here, different entities may have the same 
content.

Having the three entities (I call them &trLSep;, &trLSepAnd;, and
&trLSepAnd2; in that order), doxygen can generate any list in a
language independent form like this:

\verbatim
  item1
  item1&trLSepAnd2;item2
  item1&trLSep;item2&trLSepAnd;item3
\endverbatim

The language dependent form appears when the separator entities are
replaced by the language dependent content, later.

The whole list can be defined as the content of a generated entity,
and the \c trReimplementedFromList() call can be replaced by the entity
template application, like this:

\verbatim
  <!ENTITY list001 "item1&trLSep;item2&trLSepAnd;item3" >
  <!ENTITY txt002 <![CDATA[<?doxtpl 
                  el="trReimplementedFromList" a1="&list001;"?>]]> >
\endverbatim

(The syntax of the &txt002; may not be in the final form.)

Being defined this way, everything is still language independent.
It becomes language dependent only in time when we decide to replace
the entities by the language dependent definition of the content.
It can look like this (Dutch example):

\verbatim
  <!ENTITY trLSep ", " >
  <!ENTITY trLSepAnd  " en " >
  <!ENTITY trLSepAnd2 " en " >
  <!ENTITY trReimplementedFromList "Nieuwe implementatie van $1." >
\endverbatim

and the language-dependent result will be obtained by the
application of that template entity with argument \c &list001;, and
by replacement of the separator entities by the content.


\section DevelopmentStages Development stages

I am going to sketch roughly the steps of implementation of the
mechanism so, that the mechanism could be introduced gradually into
doxygen.  This should be the answer to \ref Q4 "the fourth question".

The development stages should include:

 - \ref StudyXML
 - \ref FinishingThis
 - \ref PreparingDoxygen 
 - \ref FindDevelopers
 - \ref DefEntityFileSyntax
 - \ref IDoxEntGenerator
 - \ref TranslatorExp
 - \ref ISimpleEnt
 - \ref ISimpleEntReplacement
 - \ref ITemplateEnt
 - \ref IApplicTemplate
 - \ref NewTranslator
 - \ref SupportScript
 - \ref RemovingTranslator 

\subsection StudyXML Gaining the XML-related knowledge

Firstly, we should learn the things that could be related to the
implementation.  The reason is to avoid reinventing the wheel and to decide
what can be reused.  The experience should be summarized in this
document so, that others could say what they think about it.


\subsection FinishingThis Finishing this document
 
Then we should put together what we know, write it down, polish it a
bit in this document.  Then the document should be sent into
Doxygen-develop mailing list, and probably also send it to all
people involved in the development.

Part of the polishing would be to remove "I think"'s and "you
could"'s and make the sentences readable for wider audience.


\subsection PreparingDoxygen Preparing Doxygen for changes

<ol>
<li>We should start with changes of current doxygen translator to
    <em>remove the exceptions</em> like \c trWriteList() and \c
    trDefinedAtLineInSourceFile() (the markers replaced in the
    core).
    
    All the methods should belong to one of the cases described in
    \ref CurrentMechanismAnalysis.  The \c TranslatorAdapter_x_y_z
    should be able to capture the changes.  Even the \c trLSep(), \c
    trLSepAnd(), and \c trLSepAnd2() can be implemented via analysis
    of the \c trWriteList() results (and optimized via storing the
    strings into member variables).  Similarly, the adapter 
    could generate template strings from methods like \c
    trDefinedAtLineInSourceFile().

<li>The \c Init() method should be introduced to the \c Translator
    class.  It will be called to load and parse the entity
    definition file.  It probably should have at least one argument
    (string) to pass the name of the requested language.

<li>\anchor solution_optimc
    \b (Solved) Methods that used \c Config_getBool() to recognize
    the C versus C++ or Java flavours were split to pairs of methods
    (this will probably be introduced in version 1.2.11).  The 
    flavour recognition was moved to the core. 
    \ref solution_optimc_skipper "(Skip the solution explanation)".

    Using \c Config_getBool() inside the Translator methods was a
    big obstackle for \ref GenerDoxEntities.  The reason was that it
    was impossible to call those translator methods with any
    arguments to obtain the entity or template string other than the
    one of the current flavour (option \c "OPTIMIZE_OUTPUT_FOR_C" in
    \c Doxyfile).
    
    \b Firstly, the translator methods that called the 
    \c Config_getBool("OPTIMIZE_OUTPUT_FOR_C") were split to pairs.
    One of the new methods works for C optimization, the other for
    the C++ optimization.  The methods were given names that do not
    cause collision with the earlier defined methods.  Even the
    original identifier was not reused, because it would break the
    translator adapters.
    
    \b Secondly, the new source files \c optimc.h and \c optimc.cpp
    were introduced to implement the \c "OPTIMIZE_OUTPUT_FOR_C"
    recognition, and to call one of the new translator methods.
    The code typically looks this way:
    
    \code
    QCString trMemberDataDocumentation()
    {
      return Config_getBool("OPTIMIZE_OUTPUT_FOR_C")
               ? theTranslator->trFieldDocumentation()
               : theTranslator->trMemberDataDocumentationCpp();
    }    
    \endcode
    
    The identifier of the function (including possible arguments) is
    the same as the one used for the older translator method.  This
    means that the core sources (i.e. not translators) could be
    changed easily.  For example, the \c classdef.cpp source
    contained earlier the following command:
    
    \code
      variableMembers.writeDocumentation(ol,name(),this,
                         theTranslator->trMemberDataDocumentation());
    \endcode

    This was changed by simply including the \c optimc.h header file
    and by removing the \c "theTranslator->" sequence.

    \code
    #include "optimc.h"
    ...
      
      variableMembers.writeDocumentation(ol,name(),this,
                         trMemberDataDocumentation());
    \endcode
    
    This way the \c "OPTIMIZE_OUTPUT_FOR_C" recognition was
    effectively moved from translators to the core code.  The
    abstract class \c Translator was updated -- older methods were
    removed, new included as required methods.  In the same time,
    the \c TranslatorEnglish was updated to implement the changes.
    
    \b Thirdly, the translator adapter classes were updated so, that
    the translators for other languages worked as if nothing
    happened. This was done by fooling them that \c
    Config_getBool("OPTIMIZE_OUTPUT_FOR_C") still work.  The trick
    is that it now does not return the status of the option from \c
    Doxyfile. Instead, it returns the value of the member variable
    of the translator adapter class.  The \c Config_getBool() is
    originally the macro defined in \c config.cpp.  Because of this,
    it must be undefined for the translators first.  Then we can write:

    \code
    #undef Config_getBool
    ...
    
    protected:
  
      bool Config_getBool(const char *)
      {
          return m_bOptimizeOutputForC;
      }
     
    private:
     
      bool m_bOptimizeOutputForC; 
    \endcode
    
    The function \c Config_getBool() takes always the value 
    "OPTIMIZE_OUTPUT_FOR_C" as the argument in the TranslatorXxxx
    classes.  The trick is to return the value of the member
    variable instead of the value defined in Doxyfile.  It must be
    accessible from the derived classes -- hence protected.
     
    The m_bOptimizeOutputForC variable defines the status for the 
    Config_getBool() method. It is used only by the adapter class --
    hence private.  We can think about setting the value of the
    variable as about rather complicated way of passing the value to
    the place where the Config_getBool() is called (inside the
    translator methods).
    
    Defining the Config_getBool() method this way means that the \e
    adapter \e method's \c Config_getBool() will be called inside
    the translator methods instead of the \e global \e function.
    The souces of language translators need not to be touched.
    
    Then, the translator adapter can implement the new translator
    methods via calling the implemented older one this way:
    
    \code
    // the old one
    virtual QCString trMemberDataDocumentation()
    {
      if (m_bOptimizeOutputForC)
      {
        return "Field Documentation"; 
      }
      else
      {
        return "Member Data Documentation"; 
      }

    }

    // the new one -- first of the pair
    virtual QCString trMemberDataDocumentationCpp()
    { 
      m_bOptimizeOutputForC = false;
      return trMemberDataDocumentation();  
    }
   
    // the new one -- second of the pair
    virtual QCString trFieldDocumentation()
    { 
      m_bOptimizeOutputForC = true;
      return trMemberDataDocumentation();
    }
    \endcode

    Notice that the \c trMemberDataDocumentation() (the old one) is
    virtual, so the language specific version will be called.
    However, translator adapter have to implement it also, because
    the new methods are implemented via calling the old one, and the
    old method is already not defined in the Translator class (pure
    virtual).  In other words, it is obsolete, but needed by the
    translator adapter.
    
    The flavour (optimized for C or C++) is not determined by the \c
    Doxyfile <em>inside the translator classes</em> any more.  This means
    that we can always call for example \c trFieldDocumentation(),
    independently on what is set in \c Doxyfile.  This is the key
    for the \ref GenerDoxEntities. The core code decides (based on
    testing the value from inside \c Doxyfile) whether it will call
    the \c trFieldDocumentation() or not.
    
    \anchor solution_optimc_skipper
    
<li>The methods like \c trCompoundReference()
    could be split. Passing the \c Classdef::Xxxx argument should be
    replaced by decision what method will be called.  
    
    There is high probability that this will be done during the step
    when the code approach will be replaced by the entity approach.
    In other words, we should do some revision of the used entities
    and of the used templates and decide what will be changed and
    what will be the names of the entities.  The conversion from the
    methods to the entities will be done 
    \ref GenerDoxEntities "automatically" via several calls to the
    method with different arguments.
</ol>


\subsection FindDevelopers Search for developers (?)
  
When publishing this document in Doxygen-develop, we should ask what
other think about the idea, who has some related knowledge, and who
could participate on the development.  (This is not clear yet.  It
may be better to start single and search for participants later,
when the task can be divided.)  Some notice could be also
sent to Doxygen-users.


\subsection DefEntityFileSyntax Syntax of files with entity definitions

The syntax of the entity definitions will follow the XML way. When
the entity defines more lines, the CDATA will be used, like this:

\verbatim
  <!ENTITY trEntityId "one-line content" >
  
  <!ENTITY trEtityMulti <![CDATA[First line of the multiline entity definition.
Second line.
Third line, etc.]]> >
\endverbatim

The CDATA syntax can be prefered also when we do not want to convert
special characters to entity references (like &lt; to &amp;lt;).

To make distribution of doxygen easier, the image of those files
could be compiled into the doxygen (as in the java script case).  At
least, it should be done for English.  Doxygen, when started, should
look for the \c *.ent files in its directory.  If they are not present
here, the files will be generated.  If they are present, no
generation will happen.  This way, newer files can be downloaded (or
the old updated manually) and they will be used instead the
compiled-in definitions.


\subsection IDoxEntGenerator Implement the generator of doxygen entities

When the syntax of the files is decided, the program mentioned in
\ref GenerDoxEntities could be polished so that it could generate
such files for all languages. 

At the beginning, it must generate reliably only some selected
entities.  Later, it will be updated depending on requirements of
further development stages.


\subsection TranslatorExp Implement the experimental translator

The first development steps should start by adding \c
TranslatorExperimental class as if it was another language translator.  In
other words, no difference should be observed by the doxygen kernel.
This \c TranslatorExperimental should gradually introduce the
mechanisms of working with the entities. I would recommend to
create is as a copy of TranslatorEnglish.

Then we should introduce some method like \c Init("dutch") that
would read the file with definitions for that language into internal
structures.  Here the file will be parsed.  Here the syntax used in
the file will be last point where the syntax of files is important
for further steps.  Here also a possible conversion of character 
encoding should happen (see the \c decode() inline in \c
TranslatorCzech and others).


\subsection ISimpleEnt Implement simple entities

We will choose some very simple \c trMethod() that returns only
one simple and constant string (i.e. with no arguments).  Its body
will test the algorithm for retrieving the string based on the name
of the related entity.

Then the code around the \c TranslatorExperimental can be slightly
changed so, that we could write something like that into \c
Doxyfile:
\verbatim
  OUTPUT_LANGUAGE = Experimental Dutch
\endverbatim

It should be very simple.  Just after instantiation of the \c
TranslatorExperimental, its \c Init("dutch") will be called.

This way, the chosen \c trMethod() becomes multilingual -- based on
the initialization.  (See \ref NewTranslator "the final goal".)

This stage should implement and test the structures for searching
for the entity content.  Also, this should help to decide the basic
syntax of the entity definitions in the text file.  Probably (with
respect to future), the best way will be to adopt the simplified XML
way.


\subsection ISimpleEntReplacement Implement a simple entity replacement

Here the mechanism of defining a simple entity using other simple
entities could be tested.  The goal will be to replace the entity
reference inside another simple entity definition so that it behaved
as if the content of the outer entity was defined without using
another simple entity reference.  The example should clarify that.
Say, we have definitions of simple entities like that:

\verbatim
<!ENTITY trClassDocGeneratedFromFile "De documentatie voor deze class is gegenereerd op grond van de volgende file:" >
<!ENTITY trClassDocGeneratedFromFiles "De documentatie voor deze class is gegenereerd op grond van de volgende files:" >
<!ENTITY trStructDocGeneratedFromFile "De documentatie voor deze struct is gegenereerd op grond van de volgende file:" >
<!ENTITY trStructDocGeneratedFromFiles "De documentatie voor deze struct is gegenereerd op grond van de volgende files:" >
<!ENTITY trUnionDocGeneratedFromFile "De documentatie voor deze union is gegenereerd op grond van de volgende file:" >
<!ENTITY trUnionDocGeneratedFromFiles "De documentatie voor deze union is gegenereerd op grond van de volgende files:" >
<!ENTITY trInterfaceDocGeneratedFromFile "De documentatie voor deze interface is gegenereerd op grond van de volgende file:" >
<!ENTITY trInterfaceDocGeneratedFromFiles "De documentatie voor deze interface is gegenereerd op grond van de volgende files:" >
<!ENTITY trExceptionDocGeneratedFromFile "De documentatie voor deze exception is gegenereerd op grond van de volgende file:" >
<!ENTITY trExceptionDocGeneratedFromFiles "De documentatie voor deze exception is gegenereerd op grond van de volgende files:" >
\endverbatim

Of course, the maintainer should be a bit lazy to type ;-)  The
argument sounds better when we say that the maintainer tries to make
the definitions consistent.  He/she can decide to introduce new
simple entity definitions to simplify the definitions of the
obligatory entities, like this:

\verbatim
<!ENTITY DeDocVoorDeze "De documentatie voor deze" >
<!ENTITY IsGegenVanFile "is gegenereerd op grond van de volgende file:" >
<!ENTITY IsGegenVanFiles "is gegenereerd op grond van de volgende files:" >

<!ENTITY trClassDocGeneratedFromFile "&DeDocVoorDeze; class &IsGegenVanFile;" >
<!ENTITY trClassDocGeneratedFromFiles "&DeDocVoorDeze; class &IsGegenVanFile;" >
<!ENTITY trStructDocGeneratedFromFile "&DeDocVoorDeze; struct &IsGegenVanFile;" >
<!ENTITY trStructDocGeneratedFromFiles "&DeDocVoorDeze; struct &IsGegenVanFiles;" >
<!ENTITY trUnionDocGeneratedFromFile "&DeDocVoorDeze; union &IsGegenVanFile;" >
<!ENTITY trUnionDocGeneratedFromFiles "&DeDocVoorDeze; union &IsGegenVanFiles;" >
<!ENTITY trInterfaceDocGeneratedFromFile "&DeDocVoorDeze; interface &IsGegenVanFile;" >
<!ENTITY trInterfaceDocGeneratedFromFiles "&DeDocVoorDeze; interface &IsGegenVanFiles;" >
<!ENTITY trExceptionDocGeneratedFromFile "&DeDocVoorDeze; exception &IsGegenVanFile;" >
<!ENTITY trExceptionDocGeneratedFromFiles "&DeDocVoorDeze; exception &IsGegenVanFiles;" >
\endverbatim

The replacement shoud be automatic.  Here the decision will be taken
and tested when the simple entity references should be expanded --
greedy or lazy evaluation of the entities ;-)


\subsection ITemplateEnt Implement template entities

Here the syntax of simple template entities will be introduced to be
stored in the entity definition text file.  Basically, the template
entity should be stored the same way as non-template entity.  The
only difference is that it contains also markers like \c $1, \c $2,
etc.  What makes the template entity differen from the other
entities will be its usage or \ref IApplicTemplate "application".

In this step, the mechanism of replacement will be implemented and
tested inside the chosen \c trMethod().  It will simply search the
markers and replace them with the arguments.


\subsection IApplicTemplate Application of template entities

In this step, the marker-replacement mechanism should be moved to
the general part of the code.  The chosen method will only prepare
the name of the template entity and the arguments.  Then it will
call the replacement code to get the string. The call may look like
this:

\verbatim
  theTranslator->DoxTpl("trReimplementedFromList", "&list001;"); 
\endverbatim

Firstly, the \c trReimplementedFromList template entity string (with
markers) will be retrieved; then the arguments will be used to
replace the markers.

As the method should return the language dependent string, the
replacement of simple entities should also be done inside.  (Think
about \c &list001; in the above example or about some other entity
reference placed in the template entity string.)

Later, when \ref NewTranslator "new Translator" will be removed, 
the application of the template entities will be implemented inside 
the replacement mechanism.  The command for the template application
can take the form of XML procesing instruction, and can be stored as
some other entity definition.  As XML processing instructions contain
special character, the CDATA block syntax is used:

\verbatim
  <!ENTITY txt002 <![CDATA[<?doxtpl 
                               tpl="&trReimplementedFromList;"
                               a1="&list001;" ?> 
                  ]]>
\endverbatim

Here, the example slightly differs from the one \ref DoxEntSummary "above". 
The syntax shown here is probably better.  The expansion of entity
references should happen first and the replacement later.  This
would give us also a possibility to use it for cases when the
template or the argument (or both) are not stored as entity
definitions:

\verbatim
  <!ENTITY txt002 <![CDATA[<?doxtpl 
                  tpl="My template with argument '$1' is used here."
                  a1="MY ARGUMENT" ?> 
                  ]]>
\endverbatim

Or the sequence can be used directly in some text, i.e. not via
referencing \c &txt002;.

It seems that the processing instructions are the right thing that
we need because they are able to capture some special functionality
(done by the named code with given arguments).  If we once move more
closely to XML, then doxtpl will be very simple external application
that just do the replacement of markers by the arguments.  This way,
general XML tools may be given language independent document sources
and the language dependent entity definitions.  The replacement will
happend during conversion (XSLT) of the sources into another form
(for example into HTML).

The reason for postponing the replacement as late as possible is,
that the \c doxtpl processing instruction may still have
language-independent form.  The language dependent form may be
produced by general XML tools.  While this may not be the goal now,
it can be handy in future.

<b>Conclusion:</b> The mechanism of the template application should
be implemented gradually this way:

 -# Explicit call of <code>theTranslator->DoxTpl()</code>, as
    shown above.
 -# Generation of the processing instruction inside the chosen
    \c trMethod() and explicit calling
    \code theTranslator->ProcInstruction("<?doxtpl ....?>");\endcode
 -# Defining new entity with the \c doxtpl processing instruction and 
    explicit calling the method which does replacement (i.e.
    processing instruction recognition will be added to the
    replacement mechanism).  So the chosen \c trMethod will define
    the procesing instruction and invoke the general replacement
    code, not the \c ProcInstruction().
 -# As the processing instruction looks like the method call (i.e.
    name and arguments), it can be generated directly of via entity
    definition into the intermediate form of the document.  The
    previously chosen \c trMethod() can be completely removed from
    the \ref NewTranslator "new translator" during the 
    \ref RemovingTranslator "translator removal" stage.
    
Concerning the last point, this can also be interpreted so, that the
previous \c trMethod() call will be replaced inside doxygen by
forming the processing instruction (deciding name of the template
and preparing arguments is similar to calling the method) and
calling the code that finally will be implemented by doxygen core.
 

\subsection NewTranslator Introduce the new, single Translator class

Finally, the \c TranslatorExperimental will implement all \c
trMethods().  In that time, it will replace the functionality of all
the other language translators and the language dependent classes
will be removed.  The language maintainers will be given the
generated language dependent files to be maintained (instead the
code).

The English entity definitions can always be loaded first, the
desired language definitions can be loaded after.  This way,
definitions for all used entities will be present.  

It could be enhanced later so, that \c adapter_x_y_z.def 
entity definitions will be loaded if some entity is not found.


\subsection SupportScript Implement the script for maintenance

The equivalent of the \c translator.pl will be implemented to
support the language maintenance.  

Firstly, how \c translator.pl is implemented.  The key here is to
analyze source code of the abstract class \c Translator in
\c Translator.h.  When we extract all pure virtual methods, we have
the list of methods that must be implemented.  Then we extract all
virtual methods from the chosen \c TransatorLanguage class.  After
comparison of the two lists we know what methods are implemented,
what are not (they are implemented by some \c TranslatorAdaper), and
what are implemented but not used (the obsolete methods).  The
status of the \c TranslatorLanguage is extracted from the identifier
of its base class.

Similarly, when the entities will be used, one of the set of
definitions (the English one) will be considered as the reference 
set.  XML comments will be used what definitions belong to what
version of doxygen.  Another form of comments will be used to 
exclude some entity identifiers from the required set.  It will name
entities that are used only for easier definition of other entities
-- see notices about \ref ISimpleEntReplacement "the replacement" of
entity references inside other entity definitions.  This way, the
script can extract the names of entities that are required by
doxygen, and since what version they are required.

From definitions of the tested language, all entity names will be
extracted (and possibly also number of replacement marks inside
template entities), and the second list will be build.  If some of
the entities are not required (not listed in the previous list), and
if references to them are not used inside definitions of the
requested entities, then they will be considered obsolete.  The
entities that are in the first list but not in the second list imply
that the language should be updated.  The minimum of the version
numbers related to the missing entities will be said to be language
maintenance status.


\subsection RemovingTranslator Remove the Translator class

Wherever doxygen core (generators) call \c Translator::trMethod(),
the code will generate the equivalent entity reference.  

The entity definitions could be redesigned. The \c trMethods() 
will be gradually removed from the new \c Translator class.

After testing, the whole generalized mechanism for working with
entities could be moved to the doxygen core (generators).  The
translator class will be completely removed.

*/ 
